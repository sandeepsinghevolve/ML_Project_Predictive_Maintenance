{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "import seaborn as sns\n",
    "from pickle import load\n",
    "from pickle import dump\n",
    "def print_scores(y, y_pred, model):\n",
    "    print(\n",
    "        'Accuracy score: {:.02%}, Precision: {:.02%}, Recall: {:.02%}, F1 score: {:.02%} '.format(\n",
    "            accuracy_score(y, y_pred),\n",
    "            precision_score(y, y_pred, pos_label=1),\n",
    "            recall_score(y, y_pred, pos_label=1),\n",
    "            f1_score(y, y_pred, pos_label=1)\n",
    "        ), model\n",
    "    )\n",
    "# Data understanding\n",
    "# Importing the dataset\n",
    "normal = pd.read_csv('Datasets/bearings/NB.csv')\n",
    "normal['Fault'] = 1\n",
    "print('Size of normal data: ',normal.shape)\n",
    "\n",
    "abnormal = pd.read_csv('Datasets/bearings/IR - 7.csv')\n",
    "abnormal['Fault'] = -1\n",
    "print('Size of abnormal data: ',abnormal.shape)\n",
    "normal.head()\n",
    "abnormal.head()\n",
    "# combine normal and abnormal data and reset index for later use\n",
    "dataset = normal.append(abnormal)\n",
    "dataset = dataset.reset_index(drop=True)\n",
    "dataset\n",
    "normal = dataset[dataset.Fault == 1]\n",
    "abnormal = dataset[dataset.Fault == -1]\n",
    "## check null and duplicates\n",
    "# Checking for null values\n",
    "features = ['DE', 'FE', 'Fault']\n",
    "N_null = sum(dataset[features].isnull().sum())\n",
    "print(\"The dataset contains {} null values\".format(N_null)) \n",
    "\n",
    "# Removing duplicates if there exist\n",
    "N_dupli = sum(dataset.duplicated(keep='first'))\n",
    "dataset = dataset.drop_duplicates(keep='first').reset_index(drop=True)\n",
    "print(\"The dataset contains {} duplicates\".format(N_dupli))\n",
    "\n",
    "# Number of samples in the dataset\n",
    "N = dataset.shape[0]\n",
    "print('Size of cleaned dataset: ', N)\n",
    "6941 duplicates are removed\n",
    "## Basic statistics\n",
    "dataset.info()\n",
    "All data type are numetrical, there is no need to encode feature\n",
    "dataset.describe()\n",
    "Statistics are visualized as plot later for better understanding\n",
    "# save table as csv to folder\n",
    "dataset.describe().to_csv(r'Statistics/anomaly-detection-bearing-statistics.csv', index = True)\n",
    "## EDA\n",
    "# Boxpot and histogram of each feature\n",
    "for (columnName, columnData) in dataset.iteritems():\n",
    "\n",
    "    # Creating an empty chart\n",
    "    fig, ((ax1, ax2)) = plt.subplots(1, 2,  figsize=(15, 4))\n",
    "\n",
    "    # Extracting the feature values\n",
    "    x = columnData\n",
    "\n",
    "    # Boxplot\n",
    "    ax1.boxplot(x)\n",
    "    ax1.set_title( 'Boxplot for {}'.format(columnName) )\n",
    "\n",
    "    # Histogram\n",
    "    ax2.hist(x, bins=20)\n",
    "    ax2.set_title( 'Histogram for {}'.format(columnName) )\n",
    "\n",
    "    # Display\n",
    "    plt.show()\n",
    "def plot_feature(data, dataName):\n",
    "    x = list(range(len(data.index)))\n",
    "    y = data\n",
    "\n",
    "    # plot the humidity data\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    plt.plot(x,y)\n",
    "    plt.ylabel(dataName)\n",
    "    plt.xlabel('Index')\n",
    "    plt.xlim((0,len(data.index)))\n",
    "for (columnName, columnData) in dataset.iteritems():\n",
    "    plot_feature(columnData, columnName)\n",
    "## Correlation matrix and heatmap\n",
    "dataset.corr()\n",
    "# save table as csv to folder\n",
    "dataset.corr().to_csv(r'Statistics/bearing-corr.csv', index = True)\n",
    "# make correlation matrix to heatmap\n",
    "f, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.heatmap(dataset.corr(), cmap='YlGnBu', vmax = .9, square = True, annot=True)\n",
    "# Data preparation\n",
    "### Feature scaling and Split data for novelty detection\n",
    "sc = StandardScaler()\n",
    "X_inliers = sc.fit_transform(normal[['DE', 'FE']])\n",
    "X = sc.transform(dataset[['DE', 'FE']])\n",
    "y = dataset.iloc[:, 2]\n",
    "X\n",
    "y\n",
    "X_inliers\n",
    "# pickle data for later use\n",
    "dump(X, open(\"pickle/dataset/bearing-anomaly-detection/X.pkl\", \"wb\"))\n",
    "dump(y, open(\"pickle/dataset/bearing-anomaly-detection/y.pkl\", \"wb\"))\n",
    "dump(X_inliers, open('pickle/dataset/bearing-anomaly-detection/X_inliers.pkl', 'wb'))\n",
    "### Feature scaling and Split data for outliers detection\n",
    "X_train, X_test = train_test_split(normal, test_size = 0.2, shuffle=False, random_state = 0)\n",
    "y_train, y_test = train_test_split(abnormal, test_size = 0.2, shuffle=False, random_state = 0)\n",
    "train = X_train.append(y_train)\n",
    "train = train.reset_index(drop=True)\n",
    "train\n",
    "test = X_test.append(y_test)\n",
    "test = test.reset_index(drop=True)\n",
    "test\n",
    "# pickle data for later use\n",
    "dump(train, open(\"pickle/dataset/bearing-anomaly-detection/train.pkl\", \"wb\"))\n",
    "dump(test, open(\"pickle/dataset/bearing-anomaly-detection/test.pkl\", \"wb\"))\n",
    "sc = StandardScaler()\n",
    "train_data = sc.fit_transform(train[['DE', 'FE']])\n",
    "test_data = sc.transform(test[['DE', 'FE']])\n",
    "dump(train_data, open(\"pickle/dataset/bearing-anomaly-detection/train_data.pkl\", \"wb\"))\n",
    "dump(test_data, open(\"pickle/dataset/bearing-anomaly-detection/test_data.pkl\", \"wb\"))\n",
    "# Novelty detection\n",
    "train with normal and test with mix => novelty detection\n",
    "## LOF, novelty detection\n",
    "# load data\n",
    "X = load(open('pickle/dataset/bearing-anomaly-detection/X.pkl', 'rb'))\n",
    "X_inliers = load(open('pickle/dataset/bearing-anomaly-detection/X_inliers.pkl', 'rb'))\n",
    "y = load(open('pickle/dataset/bearing-anomaly-detection/y.pkl', 'rb'))\n",
    "lof = LocalOutlierFactor(n_neighbors=50,novelty=True)\n",
    "lof.fit(X_inliers) \n",
    "dataset['pred_anomaly'] = lof.predict(X)\n",
    "dataset\n",
    "print_scores(y, dataset['pred_anomaly'], lof)\n",
    "pred_anomalies = dataset[dataset['pred_anomaly'] == -1]\n",
    "f, (ax1) = plt.subplots(figsize=(18, 6))\n",
    "ax1.scatter(pred_anomalies.index, pred_anomalies.DE, label='DE', color='red', s=10)\n",
    "ax1.plot(dataset.index, dataset.DE, label='DE');\n",
    "plt.xlim((0,len(dataset.index)))\n",
    "\n",
    "plt.title('Local Outlier Factor')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "# Outliers detection\n",
    "Outliers detection train with both inliers and outliers\n",
    "## Isolation forest\n",
    "# load data\n",
    "train = load(open('pickle/dataset/bearing-anomaly-detection/train.pkl', 'rb'))\n",
    "test = load(open('pickle/dataset/bearing-anomaly-detection/test.pkl', 'rb'))\n",
    "train_data = load(open('pickle/dataset/bearing-anomaly-detection/train_data.pkl', 'rb'))\n",
    "test_data = load(open('pickle/dataset/bearing-anomaly-detection/test_data.pkl', 'rb'))\n",
    "isolationForest =  IsolationForest()\n",
    "isolationForest.fit(train_data) \n",
    "train['pred_anomaly'] = isolationForest.predict(train_data)\n",
    "\n",
    "# visualization\n",
    "train_anomalies = train[train['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax1) = plt.subplots(figsize=(18, 6))\n",
    "ax1.scatter(train_anomalies.index, train_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax1.plot(train.index, train.DE, label='DE');\n",
    "plt.xlim((0,len(train.index)))\n",
    "\n",
    "plt.title('Isolation Forest')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print training scores\n",
    "print_scores(train['Fault'], train['pred_anomaly'], isolationForest)\n",
    "test['pred_anomaly'] = isolationForest.predict(test_data)\n",
    "\n",
    "# visualization\n",
    "test_anomalies = test[test['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax1) = plt.subplots(figsize=(18, 6))\n",
    "ax1.scatter(test_anomalies.index, test_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax1.plot(test.index, test.DE, label='DE');\n",
    "plt.xlim((0,len(test.index)))\n",
    "\n",
    "plt.title('Isolation Forest')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print_scores(test['Fault'], test['pred_anomaly'] , isolationForest)\n",
    "## Elliptic Envelope\n",
    "# load data\n",
    "train = load(open('pickle/dataset/bearing-anomaly-detection/train.pkl', 'rb'))\n",
    "test = load(open('pickle/dataset/bearing-anomaly-detection/test.pkl', 'rb'))\n",
    "train_data = load(open('pickle/dataset/bearing-anomaly-detection/train_data.pkl', 'rb'))\n",
    "test_data = load(open('pickle/dataset/bearing-anomaly-detection/test_data.pkl', 'rb'))\n",
    "# Elliptic Envelope\n",
    "ee =  EllipticEnvelope(random_state=0)\n",
    "train['pred_anomaly'] = ee.fit_predict(train_data)\n",
    "# visualization\n",
    "train_anomalies = train[train['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax1) = plt.subplots(figsize=(18, 6))\n",
    "ax1.scatter(train_anomalies.index, train_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax1.plot(train.index, train.DE, label='DE');\n",
    "plt.xlim((0,len(train.index)))\n",
    "\n",
    "plt.title('Elliptic Envelope')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "### print training scores\n",
    "print_scores(train['Fault'], train['pred_anomaly'], ee)\n",
    "### Predict on testing data\n",
    "test['pred_anomaly'] = ee.predict(test_data)\n",
    "\n",
    "# visualization\n",
    "test_anomalies = test[test['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax1) = plt.subplots(figsize=(18, 6))\n",
    "ax1.scatter(test_anomalies.index, test_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax1.plot(test.index, test.DE, label='DE');\n",
    "plt.xlim(0, len(test.index))\n",
    "\n",
    "plt.title('Elliptic Envelope')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "### Print the testing scores\n",
    "print_scores(test['Fault'], test['pred_anomaly'], ee)\n",
    "## One class SVM\n",
    "# load data\n",
    "train = load(open('pickle/dataset/bearing-anomaly-detection/train.pkl', 'rb'))\n",
    "test = load(open('pickle/dataset/bearing-anomaly-detection/test.pkl', 'rb'))\n",
    "train_data = load(open('pickle/dataset/bearing-anomaly-detection/train_data.pkl', 'rb'))\n",
    "test_data = load(open('pickle/dataset/bearing-anomaly-detection/test_data.pkl', 'rb'))\n",
    "svm = OneClassSVM(nu = 0.01,kernel=\"rbf\", gamma=0.01)\n",
    "pred_train = svm.fit(train_data)\n",
    "train['pred_anomaly'] = pd.Series(svm.predict(train_data))\n",
    "\n",
    "train_anomalies = train[train['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax3) = plt.subplots(figsize=(18, 6))\n",
    "ax3.scatter(train_anomalies.index, train_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax3.plot(train.index, train.DE, label='DE');\n",
    "plt.xlim((0,len(train.index)))\n",
    "\n",
    "plt.title('One Class SVM')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "### Print training scores\n",
    "print_scores(train['Fault'], train['pred_anomaly'], svm)\n",
    "### Predict on test data\n",
    "test['pred_anomaly'] = svm.predict(test_data)\n",
    "\n",
    "test_anomalies = test[test['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax3) = plt.subplots(figsize=(18, 6))\n",
    "ax3.scatter(test_anomalies.index, test_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax3.plot(test.index, test.DE, label='DE');\n",
    "plt.xlim(0, len(test.index))\n",
    "\n",
    "plt.title('One Class SVM')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print_scores(test['Fault'], test['pred_anomaly'], svm)\n",
    "### Try default nu\n",
    "# load data\n",
    "train = load(open('/Users/yi-chenlin/Desktop/Final project/pickle/dataset/bearing/train.pkl', 'rb'))\n",
    "test = load(open('/Users/yi-chenlin/Desktop/Final project/pickle/dataset/bearing/test.pkl', 'rb'))\n",
    "train_data = load(open('/Users/yi-chenlin/Desktop/Final project/pickle/dataset/bearing/train_data.pkl', 'rb'))\n",
    "test_data = load(open('/Users/yi-chenlin/Desktop/Final project/pickle/dataset/bearing/test_data.pkl', 'rb'))\n",
    "svm = OneClassSVM(kernel=\"rbf\", gamma=0.01)\n",
    "pred_train = svm.fit(train_data)\n",
    "train['pred_anomaly'] = pd.Series(svm.predict(train_data))\n",
    "\n",
    "train_anomalies = train[train['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax3) = plt.subplots(figsize=(18, 6))\n",
    "ax3.scatter(train_anomalies.index, train_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax3.plot(train.index, train.DE, label='DE');\n",
    "plt.xlim((0,len(train.index)))\n",
    "\n",
    "plt.title('One Class SVM')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print_scores(train['Fault'], train['pred_anomaly'], svm)\n",
    "test['pred_anomaly'] = svm.predict(test_data)\n",
    "\n",
    "test_anomalies = test[test['pred_anomaly'] == -1]\n",
    "\n",
    "f, (ax3) = plt.subplots(figsize=(18, 6))\n",
    "ax3.scatter(test_anomalies.index, test_anomalies.DE, label='pred_anomaly', color='red', s=10)\n",
    "ax3.plot(test.index, test.DE, label='DE');\n",
    "plt.xlim(0, len(test.index))\n",
    "\n",
    "plt.title('One Class SVM')\n",
    "plt.legend();\n",
    "plt.show();\n",
    "print_scores(test['Fault'], test['pred_anomaly'], svm)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
